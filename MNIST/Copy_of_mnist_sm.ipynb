{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKZOOdQtQ7p4",
        "colab_type": "code",
        "outputId": "e50a5037-6e83-42e7-e365-76949d6110f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from IPython.display import Image"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6GNp8aYkq99",
        "colab_type": "code",
        "outputId": "b8782bb2-a112-4d83-c25d-ba5bfac6fbd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "# MLP Architecture\n",
        "\n",
        "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\", width=500, height=250)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\" width=\"500\" height=\"250\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7bYCShOSDtR",
        "colab_type": "text"
      },
      "source": [
        "mnist데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1O3FplrOyOt",
        "colab_type": "code",
        "outputId": "b67f0101-3f5d-402e-817b-4db7419978af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF-6yYTKRDMN",
        "colab_type": "code",
        "outputId": "7e3371f0-7ad9-4b46-a0ae-e8a91cc7b3b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Oa_TjCMSjvP",
        "colab_type": "text"
      },
      "source": [
        "train 데이터 -> train, validation데이터로 나누기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoypLo6hRO1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_val = x_train[50000:60000]\n",
        "x_train = x_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHTwFKZLlGXg",
        "colab_type": "code",
        "outputId": "0122e2f9-f802-4064-af4d-ca2597bcbdf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "\n",
        "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/reshape_mnist.png\", width=500, height=250)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/reshape_mnist.png\" width=\"500\" height=\"250\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZODwo7lXTpId",
        "colab_type": "code",
        "outputId": "6f38c9b6-0278-4b76-a2cc-ac2ab90839a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# 행렬을 array로 변환\n",
        "x_train = x_train.reshape(50000, 784)\n",
        "x_val = x_val.reshape(10000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSuCutXJUSsH",
        "colab_type": "code",
        "outputId": "4a094b88-9bd8-4c74-a12f-e289ef5a0bdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
              "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
              "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
              "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
              "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
              "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
              "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
              "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
              "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
              "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7fbC-AoUV-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 정규화 (0~255까지의 숫자를 0~1로 변환, 수학에서 표준정규분포 z라 비슷하다고 보면 됨)\n",
        "x_train = x_train.astype('float32')\n",
        "x_val = x_val.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "gray_scale = 255\n",
        "x_train /= gray_scale\n",
        "x_val /= gray_scale\n",
        "x_test /= gray_scale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38XQ42sxWj-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y데이터를 one-hot encoding(10개의 노드가 있으면 그 값만 1로 바꾸고 나머지는 0)\n",
        "num_classes = 10\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kAkJ6E1YQBQ",
        "colab_type": "code",
        "outputId": "89bbad52-7ba4-4a36-e79e-23e138dfb113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "y_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc73RXwQlRhs",
        "colab_type": "code",
        "outputId": "cae0aea9-bf48-4313-ddfc-73050e6abb43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\", width=500, height=250)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\" width=\"500\" height=\"250\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5ExtZfEY2Gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(tf.float32, [None, 784]) #784개의 input을 받음\n",
        "y = tf.placeholder(tf.float32, [None, 10]) #10개의 숫자로 구분할 거야"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1iIvUhMhNhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlp(x):\n",
        "  # hidden layer1\n",
        "  w1 = tf.Variable(tf.random_uniform([784,256])) #784개의 input, 256개의 노드\n",
        "  b1 = tf.Variable(tf.zeros([256])) #256개의 바이어스(bias), bias갯수=노드갯수\n",
        "  h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
        "  # hidden layer2\n",
        "  w2 = tf.Variable(tf.random_uniform([256,128]))\n",
        "  b2 = tf.Variable(tf.zeros([128]))\n",
        "  h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
        "  # output layer\n",
        "  w3 = tf.Variable(tf.random_uniform([128,10]))\n",
        "  b3 = tf.Variable(tf.zeros([10]))\n",
        "  logits = tf.matmul(h2, w3) + b3\n",
        "\n",
        "  return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTfT-eIskGUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits = mlp(x) #ouput을 받고"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1beNUyQkPWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "    logits=logits, labels=y)) #logits value를 softmax에 받아서 prediction함\n",
        "    #prediction한 결과를 cross entropy를 minimize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPkms-EIlcGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_op)\n",
        "#loss function을 minimize, 최적화\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21JLPcHmkRNM",
        "colab_type": "code",
        "outputId": "e3e86641-6aff-49ad-db85-255d161516b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "# 조기 종료(early stopping)\n",
        "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/early_stop.png\", width=500, height=250)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/early_stop.png\" width=\"500\" height=\"250\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKChtJysmwIr",
        "colab_type": "code",
        "outputId": "7eb91d08-8335-473c-db6f-2894dc7a209a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "# 조기종료를 안 했을 때\n",
        "# initialize\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# train hyperparameters\n",
        "epoch_cnt = 30 # 돌리는 횟수\n",
        "batch_size = 1000\n",
        "iteration = len(x_train) // batch_size\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    for epoch in range(epoch_cnt):\n",
        "        avg_loss = 0.\n",
        "        start = 0; end = batch_size\n",
        "        \n",
        "        for i in range(iteration):\n",
        "            _, loss = sess.run([train_op, loss_op], \n",
        "                               feed_dict={x: x_train[start: end], y: y_train[start: end]})\n",
        "            start += batch_size; end += batch_size\n",
        "            # Compute average loss\n",
        "            avg_loss += loss / iteration\n",
        "            \n",
        "        # Validate model\n",
        "        preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
        "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
        "        # Calculate accuracy\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "        cur_val_acc = accuracy.eval({x: x_val, y: y_val})\n",
        "        print(\"epoch: \"+str(epoch)+\", validation accuracy: \" \n",
        "              + str(cur_val_acc) +', loss: '+str(avg_loss))\n",
        "    \n",
        "    # Test model\n",
        "    preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    print(\"[Test Accuracy] :\", accuracy.eval({x: x_test, y: y_test}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, validation accuracy: 0.1162, loss: 9782.427485351558\n",
            "epoch: 1, validation accuracy: 0.7973, loss: 278.3137446212769\n",
            "epoch: 2, validation accuracy: 0.875, loss: 12.431630783081053\n",
            "epoch: 3, validation accuracy: 0.8876, loss: 8.002866516113281\n",
            "epoch: 4, validation accuracy: 0.8945, loss: 6.276257386207581\n",
            "epoch: 5, validation accuracy: 0.902, loss: 5.045669846534729\n",
            "epoch: 6, validation accuracy: 0.9017, loss: 4.183398394584655\n",
            "epoch: 7, validation accuracy: 0.8983, loss: 3.7552951526641842\n",
            "epoch: 8, validation accuracy: 0.8915, loss: 3.501420512199401\n",
            "epoch: 9, validation accuracy: 0.9076, loss: 3.539449725151061\n",
            "epoch: 10, validation accuracy: 0.8844, loss: 3.220013766288758\n",
            "epoch: 11, validation accuracy: 0.902, loss: 3.2388172984123242\n",
            "epoch: 12, validation accuracy: 0.8985, loss: 3.2839224553108215\n",
            "epoch: 13, validation accuracy: 0.8914, loss: 3.1763450026512148\n",
            "epoch: 14, validation accuracy: 0.8989, loss: 3.3856840467453004\n",
            "epoch: 15, validation accuracy: 0.9129, loss: 2.775977165699004\n",
            "epoch: 16, validation accuracy: 0.9146, loss: 2.5741840219497685\n",
            "epoch: 17, validation accuracy: 0.9142, loss: 4.886209044456481\n",
            "epoch: 18, validation accuracy: 0.9001, loss: 3.611134524345398\n",
            "epoch: 19, validation accuracy: 0.8877, loss: 3.5286970663070685\n",
            "epoch: 20, validation accuracy: 0.9104, loss: 3.064694926738739\n",
            "epoch: 21, validation accuracy: 0.9088, loss: 2.6174731492996224\n",
            "epoch: 22, validation accuracy: 0.9069, loss: 2.421589314937592\n",
            "epoch: 23, validation accuracy: 0.9074, loss: 2.509313726425171\n",
            "epoch: 24, validation accuracy: 0.8959, loss: 2.5566661953926078\n",
            "epoch: 25, validation accuracy: 0.9139, loss: 2.4098639440536496\n",
            "epoch: 26, validation accuracy: 0.9013, loss: 2.22072146654129\n",
            "epoch: 27, validation accuracy: 0.891, loss: 2.429842989444733\n",
            "epoch: 28, validation accuracy: 0.9069, loss: 2.0489961290359497\n",
            "epoch: 29, validation accuracy: 0.9163, loss: 1.537286754846573\n",
            "[Test Accuracy] : 0.9152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwf-p0AVmwnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 조기종료를 했을 때\n",
        "# initialize\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Add ops to save and restore all the variables.\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# train hyperparameters\n",
        "epoch_cnt = 300\n",
        "batch_size = 1000\n",
        "iteration = len(x_train) // batch_size\n",
        "\n",
        "earlystop_threshold = 5 # 성능이 떨어질 때, 5번정도 기다린다는 의미 떨어졌다 다시 올라가는 경우도 있어서\n",
        "earlystop_cnt = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBvp8pMGkpQZ",
        "colab_type": "code",
        "outputId": "74c69cd7-b1a5-42f6-c3c4-ce29198d7b15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    prev_train_acc = 0.0\n",
        "    max_val_acc = 0.0\n",
        "    \n",
        "    for epoch in range(epoch_cnt):\n",
        "        avg_loss = 0.\n",
        "        start = 0; end = batch_size\n",
        "        \n",
        "        for i in range(iteration):\n",
        "            _, loss = sess.run([train_op, loss_op], \n",
        "                               feed_dict={x: x_train[start: end], y: y_train[start: end]})\n",
        "            start += batch_size; end += batch_size\n",
        "            # Compute train average loss\n",
        "            avg_loss += loss / iteration\n",
        "            \n",
        "        # Validate model\n",
        "        preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
        "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
        "        # Calculate accuracy\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "        # train accuracy\n",
        "        cur_train_acc = accuracy.eval({x: x_train, y: y_train})\n",
        "        # validation accuarcy\n",
        "        cur_val_acc = accuracy.eval({x: x_val, y: y_val})\n",
        "        # validation loss\n",
        "        cur_val_loss = loss_op.eval({x: x_val, y: y_val})\n",
        "        \n",
        "        print(\"epoch: \"+str(epoch)+\n",
        "              \", train acc: \" + str(cur_train_acc) +\n",
        "              \", val acc: \" + str(cur_val_acc) )\n",
        "              #', train loss: '+str(avg_loss)+\n",
        "              #', val loss: '+str(cur_val_loss))\n",
        "        \n",
        "        if cur_val_acc < max_val_acc:\n",
        "            if cur_train_acc > prev_train_acc or cur_train_acc > 0.99:\n",
        "                if earlystop_cnt == earlystop_threshold:\n",
        "                    print(\"early stopped on \"+str(epoch))\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"overfitting warning: \"+str(earlystop_cnt))\n",
        "                    earlystop_cnt += 1\n",
        "            else:\n",
        "                earlystop_cnt = 0\n",
        "        else:\n",
        "            earlystop_cnt = 0\n",
        "            max_val_acc = cur_val_acc\n",
        "            # Save the variables to file.\n",
        "            save_path = saver.save(sess, \"model/model.ckpt\")\n",
        "        prev_train_acc = cur_train_acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, train acc: 0.17326, val acc: 0.1807\n",
            "epoch: 1, train acc: 0.75264, val acc: 0.7783\n",
            "epoch: 2, train acc: 0.85696, val acc: 0.8699\n",
            "epoch: 3, train acc: 0.8728, val acc: 0.8815\n",
            "epoch: 4, train acc: 0.88684, val acc: 0.8937\n",
            "epoch: 5, train acc: 0.89436, val acc: 0.8985\n",
            "epoch: 6, train acc: 0.89746, val acc: 0.9008\n",
            "epoch: 7, train acc: 0.89718, val acc: 0.8993\n",
            "epoch: 8, train acc: 0.89722, val acc: 0.8993\n",
            "overfitting warning: 0\n",
            "epoch: 9, train acc: 0.88582, val acc: 0.8866\n",
            "epoch: 10, train acc: 0.88132, val acc: 0.8803\n",
            "epoch: 11, train acc: 0.89528, val acc: 0.8926\n",
            "overfitting warning: 0\n",
            "epoch: 12, train acc: 0.91422, val acc: 0.9132\n",
            "epoch: 13, train acc: 0.91452, val acc: 0.9113\n",
            "overfitting warning: 0\n",
            "epoch: 14, train acc: 0.90752, val acc: 0.9035\n",
            "epoch: 15, train acc: 0.90648, val acc: 0.9059\n",
            "epoch: 16, train acc: 0.92338, val acc: 0.9233\n",
            "epoch: 17, train acc: 0.90312, val acc: 0.9052\n",
            "epoch: 18, train acc: 0.87682, val acc: 0.8862\n",
            "epoch: 19, train acc: 0.91748, val acc: 0.9164\n",
            "overfitting warning: 0\n",
            "epoch: 20, train acc: 0.92244, val acc: 0.9192\n",
            "overfitting warning: 1\n",
            "epoch: 21, train acc: 0.92634, val acc: 0.9225\n",
            "overfitting warning: 2\n",
            "epoch: 22, train acc: 0.86438, val acc: 0.8673\n",
            "epoch: 23, train acc: 0.92532, val acc: 0.9242\n",
            "epoch: 24, train acc: 0.92084, val acc: 0.9128\n",
            "epoch: 25, train acc: 0.9219, val acc: 0.9135\n",
            "overfitting warning: 0\n",
            "epoch: 26, train acc: 0.92234, val acc: 0.9156\n",
            "overfitting warning: 1\n",
            "epoch: 27, train acc: 0.89424, val acc: 0.889\n",
            "epoch: 28, train acc: 0.92384, val acc: 0.9192\n",
            "overfitting warning: 0\n",
            "epoch: 29, train acc: 0.92586, val acc: 0.9211\n",
            "overfitting warning: 1\n",
            "epoch: 30, train acc: 0.9039, val acc: 0.897\n",
            "epoch: 31, train acc: 0.92466, val acc: 0.9186\n",
            "overfitting warning: 0\n",
            "epoch: 32, train acc: 0.93494, val acc: 0.9257\n",
            "epoch: 33, train acc: 0.92296, val acc: 0.9132\n",
            "epoch: 34, train acc: 0.92824, val acc: 0.9195\n",
            "overfitting warning: 0\n",
            "epoch: 35, train acc: 0.92362, val acc: 0.915\n",
            "epoch: 36, train acc: 0.93166, val acc: 0.9259\n",
            "epoch: 37, train acc: 0.94124, val acc: 0.9317\n",
            "epoch: 38, train acc: 0.92324, val acc: 0.9134\n",
            "epoch: 39, train acc: 0.92414, val acc: 0.913\n",
            "overfitting warning: 0\n",
            "epoch: 40, train acc: 0.93578, val acc: 0.924\n",
            "overfitting warning: 1\n",
            "epoch: 41, train acc: 0.92664, val acc: 0.9158\n",
            "epoch: 42, train acc: 0.92486, val acc: 0.918\n",
            "epoch: 43, train acc: 0.93802, val acc: 0.9291\n",
            "overfitting warning: 0\n",
            "epoch: 44, train acc: 0.9459, val acc: 0.9314\n",
            "overfitting warning: 1\n",
            "epoch: 45, train acc: 0.95028, val acc: 0.9328\n",
            "epoch: 46, train acc: 0.94978, val acc: 0.9345\n",
            "epoch: 47, train acc: 0.95356, val acc: 0.9347\n",
            "epoch: 48, train acc: 0.95062, val acc: 0.9309\n",
            "epoch: 49, train acc: 0.95008, val acc: 0.9294\n",
            "epoch: 50, train acc: 0.95202, val acc: 0.9302\n",
            "overfitting warning: 0\n",
            "epoch: 51, train acc: 0.95098, val acc: 0.9294\n",
            "epoch: 52, train acc: 0.96004, val acc: 0.9399\n",
            "epoch: 53, train acc: 0.94708, val acc: 0.932\n",
            "epoch: 54, train acc: 0.94094, val acc: 0.9274\n",
            "epoch: 55, train acc: 0.94728, val acc: 0.9331\n",
            "overfitting warning: 0\n",
            "epoch: 56, train acc: 0.96856, val acc: 0.9478\n",
            "epoch: 57, train acc: 0.96098, val acc: 0.9419\n",
            "epoch: 58, train acc: 0.96356, val acc: 0.9413\n",
            "overfitting warning: 0\n",
            "epoch: 59, train acc: 0.96222, val acc: 0.941\n",
            "epoch: 60, train acc: 0.96426, val acc: 0.9423\n",
            "overfitting warning: 0\n",
            "epoch: 61, train acc: 0.96624, val acc: 0.9428\n",
            "overfitting warning: 1\n",
            "epoch: 62, train acc: 0.96344, val acc: 0.9388\n",
            "epoch: 63, train acc: 0.97244, val acc: 0.9458\n",
            "overfitting warning: 0\n",
            "epoch: 64, train acc: 0.97894, val acc: 0.9502\n",
            "epoch: 65, train acc: 0.97966, val acc: 0.9514\n",
            "epoch: 66, train acc: 0.97138, val acc: 0.947\n",
            "epoch: 67, train acc: 0.9782, val acc: 0.9528\n",
            "epoch: 68, train acc: 0.976, val acc: 0.9497\n",
            "epoch: 69, train acc: 0.97816, val acc: 0.9533\n",
            "epoch: 70, train acc: 0.97652, val acc: 0.9527\n",
            "epoch: 71, train acc: 0.98128, val acc: 0.9562\n",
            "epoch: 72, train acc: 0.98126, val acc: 0.9566\n",
            "epoch: 73, train acc: 0.97594, val acc: 0.9501\n",
            "epoch: 74, train acc: 0.97784, val acc: 0.9528\n",
            "overfitting warning: 0\n",
            "epoch: 75, train acc: 0.97272, val acc: 0.949\n",
            "epoch: 76, train acc: 0.97652, val acc: 0.9525\n",
            "overfitting warning: 0\n",
            "epoch: 77, train acc: 0.9847, val acc: 0.9574\n",
            "epoch: 78, train acc: 0.97626, val acc: 0.9506\n",
            "epoch: 79, train acc: 0.98298, val acc: 0.9563\n",
            "overfitting warning: 0\n",
            "epoch: 80, train acc: 0.98754, val acc: 0.9587\n",
            "epoch: 81, train acc: 0.99052, val acc: 0.9608\n",
            "epoch: 82, train acc: 0.98572, val acc: 0.956\n",
            "epoch: 83, train acc: 0.98698, val acc: 0.9588\n",
            "overfitting warning: 0\n",
            "epoch: 84, train acc: 0.99256, val acc: 0.9626\n",
            "epoch: 85, train acc: 0.9919, val acc: 0.9624\n",
            "overfitting warning: 0\n",
            "epoch: 86, train acc: 0.98896, val acc: 0.9589\n",
            "epoch: 87, train acc: 0.99316, val acc: 0.9622\n",
            "overfitting warning: 0\n",
            "epoch: 88, train acc: 0.99646, val acc: 0.9628\n",
            "epoch: 89, train acc: 0.99838, val acc: 0.9641\n",
            "epoch: 90, train acc: 0.99892, val acc: 0.9639\n",
            "overfitting warning: 0\n",
            "epoch: 91, train acc: 0.99948, val acc: 0.9644\n",
            "epoch: 92, train acc: 0.99946, val acc: 0.9648\n",
            "epoch: 93, train acc: 0.99966, val acc: 0.9648\n",
            "epoch: 94, train acc: 0.99954, val acc: 0.9652\n",
            "epoch: 95, train acc: 0.9999, val acc: 0.9648\n",
            "overfitting warning: 0\n",
            "epoch: 96, train acc: 0.99994, val acc: 0.9649\n",
            "overfitting warning: 1\n",
            "epoch: 97, train acc: 1.0, val acc: 0.9652\n",
            "epoch: 98, train acc: 1.0, val acc: 0.9652\n",
            "epoch: 99, train acc: 1.0, val acc: 0.9653\n",
            "epoch: 100, train acc: 1.0, val acc: 0.9656\n",
            "epoch: 101, train acc: 1.0, val acc: 0.9656\n",
            "epoch: 102, train acc: 1.0, val acc: 0.9654\n",
            "overfitting warning: 0\n",
            "epoch: 103, train acc: 1.0, val acc: 0.9651\n",
            "overfitting warning: 1\n",
            "epoch: 104, train acc: 1.0, val acc: 0.9647\n",
            "overfitting warning: 2\n",
            "epoch: 105, train acc: 1.0, val acc: 0.965\n",
            "overfitting warning: 3\n",
            "epoch: 106, train acc: 1.0, val acc: 0.9652\n",
            "overfitting warning: 4\n",
            "epoch: 107, train acc: 1.0, val acc: 0.9652\n",
            "early stopped on 107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScEI8E3CkyUB",
        "colab_type": "code",
        "outputId": "a0953a03-cd44-406f-c97c-4fb29e1afd8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "# Start testing\n",
        "with tf.Session() as sess:\n",
        "    # Restore variables from disk.\n",
        "    saver.restore(sess, \"model/model.ckpt\")\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    print(\"[Test Accuracy] :\", accuracy.eval({x: x_test, y: y_test}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
            "[Test Accuracy] : 0.9654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEY3XZo6lMSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
